{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75b9a06",
   "metadata": {},
   "source": [
    "### Basic Usage\n",
    "\n",
    "Using prebuilt react agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b82af37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#ChatGoogleGenerativeAI does not yet have reliable structured tool \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    google_api_key= os.getenv(\"GOOGLE_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0d582",
   "metadata": {},
   "source": [
    "What we did in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccc1e02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def magic_function(input: int) -> int:\n",
    "    \"\"\"Applies a magic function to an input.\"\"\"\n",
    "    return input + 2\n",
    "\n",
    "\n",
    "tools = [magic_function]\n",
    "\n",
    "\n",
    "query = \"what is the value of magic_function(3)?\"\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "response = llm_with_tools.invoke(query)\n",
    "\n",
    "response.tool_calls\n",
    "\n",
    "\n",
    "#Agents exist in langchain but it is not used anymore\n",
    "#agent = create_tool_calling_agent(model, tools, prompt)\n",
    "#agent_executor = AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfedf24a",
   "metadata": {},
   "source": [
    "How to do it with react agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bc349ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "556f3bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the value of magic_function(3)?',\n",
       " 'output': [HumanMessage(content='what is the value of magic_function(3)?', additional_kwargs={}, response_metadata={}, id='58a69531-dba1-44e4-add6-584ccfb4087a'),\n",
       "  AIMessage(content=\"I can call the magic_function with 3 as input, but I don't know the value it will return. Would you like me to call it?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'models/gemini-2.5-flash-preview-04-17', 'safety_ratings': []}, id='run-87d34de9-1400-432b-8b0e-2eadda1031ed-0', usage_metadata={'input_tokens': 58, 'output_tokens': 33, 'total_tokens': 278, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(llm, tools, prompt = prompt)\n",
    "\n",
    "\n",
    "messages = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})\n",
    "{\n",
    "    \"input\": query,\n",
    "    \"output\": messages[\"messages\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c3ee758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the value of magic_function(3)?',\n",
       " 'output': 'The value of magic_function(3) is 5.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})\n",
    "{\n",
    "    \"input\": query,\n",
    "    \"output\": messages[\"messages\"][-1].content,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d0c0050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Pardon?',\n",
       " 'output': 'I used the `magic_function` tool with the input `3`. The tool returned the output value of `5`. So, the value of `magic_function(3)` is 5.'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_history = messages[\"messages\"]\n",
    "\n",
    "new_query = \"Pardon?\"\n",
    "\n",
    "messages = langgraph_agent_executor.invoke(\n",
    "    {\"messages\": message_history + [(\"human\", new_query)]}\n",
    ")\n",
    "{\n",
    "    \"input\": new_query,\n",
    "    \"output\": messages[\"messages\"][-1].content,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c7c0a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent > chain:call_model] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant\\nHuman: what is the value of magic_function(3)?\\nAI: {'name': 'magic_function', 'arguments': '{\\\"input\\\": 3.0}'}\\nTool: 5\\nAI: The value of magic_function(3) is 5.\\nHuman: call the tool magic_function(3)\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [1.05s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I have already called the `magic_function` with input 3 for you. The result was 5.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"models/gemini-2.5-flash-preview-04-17\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I have already called the `magic_function` with input 3 for you. The result was 5.\",\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"models/gemini-2.5-flash-preview-04-17\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8defb7f0-830b-4e90-82c7-bac6665d9be3-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 112,\n",
      "              \"output_tokens\": 23,\n",
      "              \"total_tokens\": 227,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent > chain:RunnableSequence] [1.06s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent > chain:call_model] [1.06s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent > chain:should_continue] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent > chain:should_continue] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"__end__\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:agent] [1.06s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph] [1.07s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_verbose\n",
    "from langchain.globals import set_debug \n",
    "\n",
    "set_verbose(False)\n",
    "set_debug(True)\n",
    "\n",
    "messages = langgraph_agent_executor.invoke(\n",
    "    {\"messages\": message_history + [(\"human\", \"call the tool magic_function(3)\")]}\n",
    ")\n",
    "{\n",
    "    \"input\": \"call the tool magic_function(3)\",\n",
    "    \"output\": messages[\"messages\"][-1].content,\n",
    "}\n",
    "\n",
    "set_debug(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9e76b",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "persistence and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f815adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "system_message = \"You are a helpful assistant.\"\n",
    "\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(\n",
    "    llm, tools, prompt=system_message, checkpointer=memory\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"test-thread\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "904b4491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Utku, the output of the magic_function of 3 is 5.\n",
      "---\n",
      "Yes, I remember that your name is Utku.\n",
      "---\n",
      "The output of the magic_function of 3 was 5.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    langgraph_agent_executor.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\"user\", \"Hi, My name is Utku! What's the output of magic_function of 3?\")\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )[\"messages\"][-1].content\n",
    ")\n",
    "print(\"---\")\n",
    "print(\n",
    "    langgraph_agent_executor.invoke(\n",
    "        {\"messages\": [(\"user\", \"Remember my name?\")]}, config\n",
    "    )[\"messages\"][-1].content\n",
    ")\n",
    "print(\"---\")\n",
    "print(\n",
    "    langgraph_agent_executor.invoke(\n",
    "        {\"messages\": [(\"user\", \"what was that output again?\")]}, config\n",
    "    )[\"messages\"][-1].content\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541161b9",
   "metadata": {},
   "source": [
    "### Basic Graphs\n",
    "\n",
    "Building a basic chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ee331",
   "metadata": {},
   "source": [
    "* StateGraph: object that defines the structure of our chatbot and saves the \"states\"\n",
    "\n",
    "* Nodes: represents the llm and tools that our chatbot will call\n",
    "\n",
    "* Edges: specifies how bot transition between nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a2e3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009d2899",
   "metadata": {},
   "source": [
    "Our graph can now handle two key tasks:\n",
    "\n",
    "Each node can receive the current State as input and output an update to the state.\n",
    "Updates to messages will be appended to the existing list rather than overwriting it, thanks to the prebuilt add_messages function used with the Annotated syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8628d73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2a378e86590>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": llm.invoke(state[\"messages\"])}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "951f8f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2a378e86590>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bca5d7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03570c3c",
   "metadata": {},
   "source": [
    "running the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aba77544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        for step in graph.stream(\n",
    "            {\"messages\": user_input}, stream_mode=\"updates\"\n",
    "        ):\n",
    "            print(\"User: \"+ user_input)\n",
    "            for value in step.values():\n",
    "                print(\"Assistant: \" + value[\"messages\"].content)\n",
    "\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18ab7b",
   "metadata": {},
   "source": [
    "### Adding tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b124d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2a377e70590>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers .\"\"\"\n",
    "    return a*b\n",
    "\n",
    "tools = [multiply]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71217601",
   "metadata": {},
   "source": [
    "Then we create a class that will check if the message contains a tool call\n",
    "\n",
    "We will later replace this with LangGraph's prebuilt ToolNode to speed things up, but building it ourselves first is instructive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e65394b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2a377e70590>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "tool_node = BasicToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae4d60",
   "metadata": {},
   "source": [
    "Now we have to add an edge that will route to this node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d79cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_tools(state: State,):\n",
    "   \n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    \n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\" # the name of the node\n",
    "    return END \n",
    "\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "\n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b305396f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af3514",
   "metadata": {},
   "source": [
    "Running it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ee2d345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hi\n",
      "[AIMessage(content='Hello! How can I help you today? I can multiply two integers for you.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'models/gemini-2.5-flash-preview-04-17', 'safety_ratings': []}, id='run-18959188-8896-4e2e-be23-ee05f705b2a7-0', usage_metadata={'input_tokens': 49, 'output_tokens': 17, 'total_tokens': 66, 'input_token_details': {'cache_read': 0}})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "].\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        for step in graph.stream(\n",
    "            {\"messages\": user_input}, stream_mode=\"updates\"\n",
    "        ):\n",
    "            print(\"User: \"+ user_input)\n",
    "            for value in step.values():\n",
    "                print(value[\"messages\"])\n",
    "\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dff2ca",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83094146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f665e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "136fcc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c524a907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there! My name is Will.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Will, it's nice to meet you! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "user_input = \"Hi there! My name is Will.\"\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c05d8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, I remember! Your name is Will. How can I help you today, Will?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Remember my name?\"\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a90ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='Hi there! My name is Will.', additional_kwargs={}, response_metadata={}, id='58bd699b-f968-4f2b-a018-4be17087a293'), AIMessage(content=\"Hi Will, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'models/gemini-2.5-flash-preview-04-17', 'safety_ratings': []}, id='run-82e75c05-83fb-41e8-8581-db8668fdf93b-0', usage_metadata={'input_tokens': 56, 'output_tokens': 18, 'total_tokens': 108, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='b0aafe63-d0c0-4787-a771-0a856da2d1ec'), AIMessage(content='Yes, I remember! Your name is Will.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'models/gemini-2.5-flash-preview-04-17', 'safety_ratings': []}, id='run-5d240e82-a852-4acd-a87c-13ea23452d6f-0', usage_metadata={'input_tokens': 80, 'output_tokens': 10, 'total_tokens': 163, 'input_token_details': {'cache_read': 0}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f026a00-2c8b-6db4-8004-fa01570f5e36'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content='Yes, I remember! Your name is Will.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'models/gemini-2.5-flash-preview-04-17', 'safety_ratings': []}, id='run-5d240e82-a852-4acd-a87c-13ea23452d6f-0', usage_metadata={'input_tokens': 80, 'output_tokens': 10, 'total_tokens': 163, 'input_token_details': {'cache_read': 0}})]}}, 'step': 4, 'parents': {}, 'thread_id': '1'}, created_at='2025-05-01T15:21:53.181842+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f026a00-23a1-663d-8003-6fc01932c196'}}, tasks=())"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24462277",
   "metadata": {},
   "source": [
    "### Customizable State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08d48322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    name: str\n",
    "    birthday: str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d17342c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "\n",
    "@tool\n",
    "def human_assistance(\n",
    "    name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt(\n",
    "        {\n",
    "            \"question\": \"Is this correct?\",\n",
    "            \"name\": name,\n",
    "            \"birthday\": birthday,\n",
    "        },\n",
    "    )\n",
    "    # If the information is correct, update the state as-is.\n",
    "    if human_response.get(\"correct\", \"\").lower().startswith(\"y\"):\n",
    "        verified_name = name\n",
    "        verified_birthday = birthday\n",
    "        response = \"Correct\"\n",
    "    # Otherwise, receive information from the human reviewer.\n",
    "    else:\n",
    "        verified_name = human_response.get(\"name\", name)\n",
    "        verified_birthday = human_response.get(\"birthday\", birthday)\n",
    "        response = f\"Made a correction: {human_response}\"\n",
    "\n",
    "    # This time we explicitly update the state with a ToolMessage inside\n",
    "    # the tool.\n",
    "    state_update = {\n",
    "        \"name\": verified_name,\n",
    "        \"birthday\": verified_birthday,\n",
    "        \"messages\": [ToolMessage(response, tool_call_id=tool_call_id)],\n",
    "    }\n",
    "    # We return a Command object in the tool to update our state.\n",
    "    return Command(update=state_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12593e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [human_assistance]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    assert len(message.tool_calls) <= 1\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "13d3d3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "John might be born in 12/04/2000 What is John's birthday? When you have the answer, use the human_assistance tool for review.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I understand you mentioned John might be born on 12/04/2000 and asked for his birthday. Since you provided a potential date and asked for the confirmed birthday, I will use the `human_assistance` tool to confirm or find the correct date. I will use the date you provided as the potential birthday in the tool.\n",
      "Tool Calls:\n",
      "  human_assistance (3ad7a024-424d-4219-9f0f-4449fb443096)\n",
      " Call ID: 3ad7a024-424d-4219-9f0f-4449fb443096\n",
      "  Args:\n",
      "    birthday: 12/04/2000\n",
      "    name: John\n"
     ]
    }
   ],
   "source": [
    "user_input = (\n",
    "    \"John might be born in 12/04/2000 \"\n",
    "    \"What is John's birthday? \"\n",
    "    \"When you have the answer, use the human_assistance tool for review.\"\n",
    ")\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7088f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I understand you mentioned John might be born on 12/04/2000 and asked for his birthday. Since you provided a potential date and asked for the confirmed birthday, I will use the `human_assistance` tool to confirm or find the correct date. I will use the date you provided as the potential birthday in the tool.\n",
      "Tool Calls:\n",
      "  human_assistance (3ad7a024-424d-4219-9f0f-4449fb443096)\n",
      " Call ID: 3ad7a024-424d-4219-9f0f-4449fb443096\n",
      "  Args:\n",
      "    birthday: 12/04/2000\n",
      "    name: John\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: human_assistance\n",
      "\n",
      "Made a correction: {'name': 'John', 'birthday': 'Jan 17, 1999'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "After checking with a human, it seems John's birthday is actually Jan 17, 1999.\n"
     ]
    }
   ],
   "source": [
    "human_command = Command(\n",
    "    resume={\n",
    "        \"name\": \"John\",\n",
    "        \"birthday\": \"Jan 17, 1999\",\n",
    "    },\n",
    ")\n",
    "\n",
    "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fc6cb571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'John', 'birthday': 'Jan 17, 1999'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "\n",
    "{k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f6350",
   "metadata": {},
   "source": [
    "# LangGraph Server\n",
    "\n",
    "Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\n",
    "\n",
    "Also cloud option available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da8ffa0",
   "metadata": {},
   "source": [
    "### LangGraph CLI\n",
    "\n",
    "Creating a LangGraph app\n",
    "\n",
    "\"langgraph new path/to/your/app --template react-agent-python\"\n",
    "\n",
    "\"langgraph dev\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6462154",
   "metadata": {},
   "source": [
    "### LangGraph Studio Web UI\n",
    "\n",
    "LangGraph Studio Web is a specialized UI that you can connect to LangGraph API server to enable visualization, interaction, and debugging of your application locally. Test your graph in the LangGraph Studio Web UI by visiting the URL provided in the output of the langgraph dev command.\n",
    "\n",
    "LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70118d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "All connection attempts failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpx\\_transports\\default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpcore\\_async\\connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpcore\\_async\\connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpcore\\_async\\connection.py:124\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_backend.connect_tcp(**kwargs)\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpcore\\_backends\\auto.py:31\u001b[39m, in \u001b[36mAutoBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_backend()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.connect_tcp(\n\u001b[32m     32\u001b[39m     host,\n\u001b[32m     33\u001b[39m     port,\n\u001b[32m     34\u001b[39m     timeout=timeout,\n\u001b[32m     35\u001b[39m     local_address=local_address,\n\u001b[32m     36\u001b[39m     socket_options=socket_options,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:113\u001b[39m, in \u001b[36mAnyIOBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    108\u001b[39m exc_map = {\n\u001b[32m    109\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[32m    110\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    111\u001b[39m     anyio.BrokenResourceError: ConnectError,\n\u001b[32m    112\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfail_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph_sdk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_client\n\u001b[32m      3\u001b[39m client = get_client(url=\u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:2024\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m client.runs.stream(\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Threadless run\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33magent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# Name of assistant. Defined in langgraph.json.\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28minput\u001b[39m={\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\n\u001b[32m     10\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat is LangGraph?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m         }],\n\u001b[32m     13\u001b[39m     },\n\u001b[32m     14\u001b[39m     stream_mode=\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m ):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceiving new event of type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk.event\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(chunk.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langgraph_sdk\\client.py:332\u001b[39m, in \u001b[36mHttpClient.stream\u001b[39m\u001b[34m(self, path, method, json, params, headers)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m headers:\n\u001b[32m    330\u001b[39m     request_headers.update(headers)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.stream(\n\u001b[32m    333\u001b[39m     method, path, headers=request_headers, content=content, params=params\n\u001b[32m    334\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m res:\n\u001b[32m    335\u001b[39m     \u001b[38;5;66;03m# check status\u001b[39;00m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    337\u001b[39m         res.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\contextlib.py:210\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpx\\_client.py:1583\u001b[39m, in \u001b[36mAsyncClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1560\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1561\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m   1562\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1568\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m   1569\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1570\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m   1571\u001b[39m     method=method,\n\u001b[32m   1572\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1581\u001b[39m     extensions=extensions,\n\u001b[32m   1582\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1583\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(\n\u001b[32m   1584\u001b[39m     request=request,\n\u001b[32m   1585\u001b[39m     auth=auth,\n\u001b[32m   1586\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1587\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1588\u001b[39m )\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1590\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpx\\_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpx\\_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpx\\_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpx\\_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpx\\_transports\\default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    156\u001b[39m     value = typ()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\anaconda3\\envs\\langchain\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed"
     ]
    }
   ],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "client = get_client(url=\"http://localhost:2024\")\n",
    "\n",
    "async for chunk in client.runs.stream(\n",
    "    None,  # Threadless run\n",
    "    \"agent\", # Name of assistant. Defined in langgraph.json.\n",
    "    input={\n",
    "        \"messages\": [{\n",
    "            \"role\": \"human\",\n",
    "            \"content\": \"What is LangGraph?\",\n",
    "        }],\n",
    "    },\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    print(chunk.data)\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
